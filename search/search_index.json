{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to DocForensics","text":"<p>DocForensics is a cutting-edge solution designed to localize and identify various forms of document digital tampering and malicious alterations. Our solution aids in unveiling copy-move, splicing, erasing, and text redaction attacks, providing a reliable foundation for digital document verification and integrity assurance.</p> <p></p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Copy-Move Detection: Identify and localize instances of copy-move forgery within digital documents.</li> <li>Splicing Detection: Detect splicing attacks aimed at merging content from different sources.</li> <li>Erasing Detection: Uncover erasing attempts to remove original content from documents.</li> <li>Masking Detection: Identify areas within an image or document where a solid or patterned overlay has been applied to conceal underlying content, such as in text redaction or image obscuration.</li> <li>Inpainting Detection: Identify and localize instances where missing or altered areas within a document have been filled in with generated content to create a seamless appearance.</li> </ul>"},{"location":"api_documentation/","title":"API Documentation","text":""},{"location":"api_documentation/#api-documentation","title":"API Documentation","text":""},{"location":"api_documentation/#base-url","title":"Base URL:","text":"<p>http://0.0.0.0:6003/docs</p>"},{"location":"api_documentation/#endpoint-post-apipredict","title":"Endpoint: POST <code>/api/predict</code>","text":""},{"location":"api_documentation/#description","title":"Description:","text":"<p>This endpoint processes uploaded image files or Base64-encoded image data for forensic analysis. It returns detailed insights about the image, including potential tampering and object detection results.</p>"},{"location":"api_documentation/#request-body","title":"Request Body:","text":"<ul> <li> <p><code>multipart/form-data</code></p> <ul> <li><code>image_file</code>: (<code>string($binary), optional</code>) - The image file. Default None.</li> <li><code>base64_data</code>: (<code>string</code>, optional) - Base64 data of the image. Default None.</li> <li><code>file_name</code>: (<code>string</code>, optional) - filename for the image.</li> <li><code>transaction_id</code>: (<code>string</code>, required) - A unique identifier for the transaction.</li> </ul> </li> <li> <p>Note:</p> <p>Either <code>image_file</code> or <code>base64_data</code> must be provided, but not both; omitting or providing both will result in a error message.</p> </li> </ul>"},{"location":"api_documentation/#responses","title":"Responses:","text":""},{"location":"api_documentation/#200-ok-successful-response","title":"<code>200 OK</code>: Successful Response","text":"<pre><code>- Media Type: `application/json`\n- Example Value:\n    ```json\n    {\n        \"respcode\": 200,\n        \"respdesc\": \"Success\",\n        \"data\":  {\n            \"label\": \"tampered\",\n            \"data_stats\": {\n                \"mask_percentage\": 0.8594512939453125,\n                \"img_width\": 512,\n                \"img_height\": 512,\n                \"total_pixels\": 262144,\n                \"total_non_zero_pixels\": 2253\n            },\n            \"transaction_id\": \"1234\",\n            \"out_mask\": \"base-64-data\",\n            \"out_color_mask\": \"base-64-data\",\n            \"doc_integrity_verification_result\": {\n                \"is_logo_detected\": true,\n                \"is_flag_detected\": true,\n                \"is_hologram_detected\": true,\n                \"is_crop_detected\": false,\n                \"is_barcode_detected\": false,\n                \"logo_detection_confidence\": 0.9630905389785767,\n                \"flag_detection_confidence\": 0.9408931136131287,\n                \"hologram_detection_confidence\": 0.9314528107643127,\n                \"spiral_detection_confidence\": 0.6021785736083984,\n                \"is_spiral_lines_detected\": false\n            }\n        }\n    }\n    ```\n</code></pre>"},{"location":"api_documentation/#4xx-and-5xx-error-codes-common-error-responses","title":"<code>4XX</code> and <code>5XX</code> Error Codes: Common Error Responses","text":"<ul> <li>Media Type: <code>application/json</code></li> <li>Example for validation errors:     <pre><code>{\n    \"respcode\": 422,\n    \"respdesc\": \"Validation error details\",\n    \"data\": null\n}\n</code></pre></li> <li>Example for other error codes:     <pre><code>{\n    \"respcode\": 500,\n    \"respdesc\": \"Internal server error\",\n    \"data\": null\n}\n</code></pre></li> </ul>"},{"location":"api_documentation/#usage","title":"Usage:","text":"<p>To use this endpoint, you can either upload an image file directly or provide the image data as Base64 code. Additionally, you can optionally provide a filename and a required transaction ID for tracking purposes. Upon successful processing, the endpoint will return a JSON response containing the forensic analysis results.</p>"},{"location":"challenges/","title":"Challenges","text":"<p>Document image forensics is a challenging field due to the nuances of document manipulations and the advancement of forgery techniques. Some of the primary challenges faced in this area include:</p> <ol> <li> <p>Sophisticated Editing Tools: Modern software provides advanced tools that make manipulations seamless, making detection difficult.</p> </li> <li> <p>Variability of Documents: Different documents have varying layouts, styles, fonts, and formats, which adds to the complexity of forgery detection.</p> </li> <li> <p>High-Quality Reproductions: With advancements in printing and scanning technologies, forgers can produce high-quality replicas of original documents.</p> </li> <li> <p>Loss of Image Quality: Multiple generations of copying, rescanning, and compressing can degrade the image, obscuring traces of manipulation.</p> </li> <li> <p>Source Camera Identification: For digital photos of documents, identifying the source camera can be challenging, especially if metadata is tampered with.</p> </li> <li> <p>Machine Learning and AI: Advanced algorithms can now generate synthetic images or tamper with existing ones in a way that's hard for traditional methods to detect.</p> </li> <li> <p>Digital vs. Physical Tampering: Differentiating between tampering done on a physical document before scanning versus manipulation of the digital image after scanning can be difficult.</p> </li> <li> <p>Consistency Checking: Determining the consistency of light, shadows, and perspectives requires a detailed understanding of the document's original environment.</p> </li> <li> <p>Scale and Complexity: Automated solutions must process a vast number of documents rapidly, yet many forensic techniques can be computationally intensive.</p> </li> <li> <p>Lack of Ground Truth: For training detection models or validating methods, there is often a lack of a comprehensive dataset of forged documents with ground truth annotations.</p> </li> <li> <p>Global Variability: Official documents, like passports or IDs, vary significantly from one country or institution to another, making standardized verification challenging.</p> </li> <li> <p>Evolution of Forgery Techniques: As forensic methods improve, so do forgery techniques, leading to an ongoing cat-and-mouse game.</p> </li> <li> <p>Lack of Standardization: There isn't always a standardized procedure or benchmark for different forensic tasks, leading to variations in performance evaluations.</p> </li> <li> <p>Ethical and Privacy Concerns: In the age of data privacy, accessing or storing certain document details for forensic purposes can raise ethical and legal concerns.</p> </li> <li> <p>Loss of Integrity:</p> <ul> <li>Natural Aging: Caused by the natural aging of the document over time.</li> <li>Fading of Ink: Due to exposure to light, age, or chemical interactions.</li> <li>Degradation of Paper: Resulting from environmental conditions or poor-quality materials.</li> <li>Detection Difficulty: Reduced clarity and faded features in worn-out documents.</li> <li>Digital Restoration Artifacts: Digital enhancement efforts can unintentionally introduce or hide important details.</li> <li>Advanced Imaging Challenges: The need for specialized imaging methods to discern faded or hidden details.</li> <li>Variable Degradation: Different sections of the document may degrade at varied rates, affecting uniformity.</li> <li>Background Interference: Grain, patterns, or imperfections in paper can overshadow key content.</li> <li>Chemical Alterations: Age or environmental factors can change the chemical properties of ink or paper.</li> <li>Environmental Damage: Including water/moisture damage causing smudging and ink bleed, and burn/heat damage leading to charing or warping.</li> <li>Fragmentation Issues: Missing pieces or fragments from torn documents.</li> <li>False Positives: Natural wear and tear, or other benign changes, can sometimes resemble tampering or alterations.</li> </ul> </li> <li> <p>Document Image Quality:</p> <ul> <li>Resolution: The clarity of the document, determining how well details can be seen. Low resolution might omit important details.  </li> <li>Noise: Random variations in brightness or color information. High noise levels can mask fine details and introduce unwanted artifacts.</li> <li>Compression Artifacts: Unwanted patterns or detail degradation resulting from image compression, especially noticeable with aggressive lossy compression.</li> <li>Sharpness: The clarity of edges and details. Blurred images might hide alterations or make genuine details appear altered.</li> <li>Brightness and Contrast Variations: Uneven or incorrect brightness and contrast levels can lead to distorted colors or exaggerated details.</li> <li>Moire Patterns: Unwanted patterns that arise when scanning printed materials, interfering with the original content of the document.</li> <li>Distortions: Any warping, stretching, or perspective changes that alter the original appearance of the document.</li> <li>Background Noise: Extraneous information or artifacts that can obscure the genuine content of the document.</li> <li>Non-uniform Illumination: Shadows, highlights, or uneven lighting conditions that can distort the appearance of a document.</li> <li>Scanner/Camera Quality: The fidelity with which a capture device reproduces the document. Low-quality devices may not capture details accurately.</li> <li>Environmental Factors: Issues like glare, reflections, and other environmental elements that can impact the quality during document capture.</li> </ul> </li> </ol> <p>To overcome these challenges, ongoing research, collaboration, and continuous tool development are crucial. The integration of machine learning and traditional forensic techniques is also an exciting avenue, leading to improved detection rates and faster processing.</p>"},{"location":"dataset_composition/","title":"Dataset Composition","text":""},{"location":"dataset_composition/#dataset-composition","title":"Dataset Composition","text":"<p>To train our model to recognize manipulations, we provide it with several hundred thousand images, both genuine and manipulated. Each image is paired with its ground truth mask. During the training process, the model learns to recognize patterns and subtle hints that suggest an image has been tampered with.</p>"},{"location":"dataset_composition/#genuine-images","title":"Genuine Images","text":"<p>These are untouched, original images. The ground truth mask for these images is purely black, indicating no regions of manipulation.</p>"},{"location":"dataset_composition/#manipulated-images","title":"Manipulated Images","text":"<p>These images have undergone some form of tampering. Accompanying each manipulated image is its ground truth mask, where:</p> <ul> <li>White regions: Highlight areas of the image that have been altered or tampered with.</li> <li>Black regions: Signify areas of the image that remain untouched or genuine.</li> </ul>"},{"location":"features/","title":"Forensic Features","text":"<p>The Document Forensic Project is equipped with several forensic features designed to detect and analyze digital manipulations within documents. Each feature focuses on a particular type of forgery, providing a detailed analysis to help verify the authenticity of digital documents.</p> <ul> <li>Forensic Features<ul> <li>Splicing Detection</li> <li>Copy-Move Detection</li> <li>Erasing Detection</li> <li>Masking Detection</li> <li>Inpainting Detection</li> </ul> </li> </ul>"},{"location":"features/#splicing-detection","title":"Splicing Detection","text":"<p>Image splicing (or Compositing) involves cutting a portion of one image and pasting it onto another. In the context of document forgery, it's taking a part of one official document and merging it with another to create a deceptive document.</p> <ul> <li>Example 1: Combining the photo from a stolen passport with the details page of a different, valid passport.</li> <li> <p>Example 2: Merging the signature from one driving license onto another to make the latter appear genuine.</p> </li> <li> <p>Example 3: Combining Two Passports:</p> <ul> <li>Scenario: A person has an expired passport with a valid visa and a new passport without that visa. They want to use the valid visa from the expired passport while traveling with their new passport.</li> <li>Splicing Action: The forger cuts out the visa page or stamp from the expired passport and pastes it onto a page in the new passport. It's made to look seamless, as if the visa was genuinely issued in the newer passport.</li> <li>Real-world Consequence: This allows the person to travel based on a visa that isn't actually linked to their current passport. If undetected, they can utilize an expired visa as if it were still valid.</li> </ul> </li> <li> <p>Example 4: Merging Different Driving Licenses:</p> <ul> <li>Scenario: Someone has a driving license with a bad driving record (e.g., DUIs or major traffic offenses) and acquires another license under a different identity with a clean record. However, they prefer the photo from the original license.</li> <li>Splicing Action: The forger takes the photo from the original license (with the bad record) and splices it onto the new license (with the clean record).</li> <li>Real-world Consequence: The individual can present a license with a clean driving record but with their preferred photo, potentially evading legal consequences or higher insurance rates linked to their actual driving history.</li> </ul> </li> <li> <p>Example 5: Manipulating National ID Expiry Dates:</p> <ul> <li>Scenario: Someone's national ID card is about to expire, which could limit certain privileges like voting.</li> <li>Splicing Action: The forger finds someone else's national ID card that has a more distant expiry date, cuts out just the date, and splices it over the expiry date on the original ID.</li> <li>Real-world Consequence: This alteration might allow someone to use an expired ID as if it's still valid, potentially engaging in civic duties or accessing services under false pretenses.</li> </ul> </li> </ul> <p> </p> Example: The individual's actual face has been substituted with a different face."},{"location":"features/#copy-move-detection","title":"Copy-Move Detection","text":"<p>Copy-move forgery is a type of manipulation where a part/segment of the document (or image) is copied and pasted into another location within the same document, often to replicate or hide specific elements. Think of it as a digital \"cut and paste\" within the same image.</p> <ul> <li>Example 1: In a passport, copying the holographic emblem and placing it onto a forged version to enhance its perceived authenticity.</li> <li> <p>Example 2: Duplicating a legitimate visa stamp on a page of a passport where an expired or rejected visa was previously.</p> </li> <li> <p>Example 3: Passport Number Manipulation:</p> <ul> <li>Scenario: A forger wants to change their passport number to avoid detection or link to past records.</li> <li>Copy-Move Action: The forger copies some digits from the birth date or expiry date sections of their passport and pastes them onto the passport number, thereby altering it.</li> <li>Real-world Consequence: If undetected, the individual might bypass certain checks at immigration or during other verification processes.</li> </ul> </li> <li> <p>Example 4: Driver's License Issue Date Alteration:</p> <ul> <li>Scenario: A young driver wants to appear as if they've had their driver's license for a longer period than they actually have, possibly to reduce insurance premiums or qualify for jobs requiring longer driving experience.</li> <li>Copy-Move Action: The forger carefully copies digits from the birthdate section or other parts of the driver's license and pastes them over the issue date, making it seem as though the license was issued earlier than it actually was.</li> <li>Real-world Consequence: The person could unlawfully get benefits or positions that require more extended driving experience or a specific age.</li> </ul> </li> </ul> <p> </p> Example: The individual's face has been copied and pasted to a different location within the same document."},{"location":"features/#erasing-detection","title":"Erasing Detection","text":"<p>This involves removing certain elements from an image. In the context of digital image editing, erasing would mean removing specific pixels representing an object or detail to either leave a blank space or replace them with a uniform background.</p> <ul> <li>Example 1: Removing an expiry date from a driving license and using the background pattern to fill in the space seamlessly.</li> <li>Example 2: Erasing a restriction or endorsement from a driving license and cloning adjacent background areas to cover the alteration.</li> </ul> <p> </p> Example: The ID number and signature have been erased from the document."},{"location":"features/#masking-detection","title":"Masking Detection","text":"<p>Covering up parts of an image or document with a solid or patterned overlay to hide underlying content.</p> <ul> <li>Example 1: To hide a person's Social Security Number (SSN) on a copied version of a national ID, a black box is placed over the number.</li> <li>Example 2: In a publicly released document, portions of the text are covered with redaction bars to prevent specific details from being disclosed.</li> </ul> <p> </p> Example: Text redaction has been applied to the image to conceal significant details."},{"location":"features/#inpainting-detection","title":"Inpainting Detection","text":"<p>Inpainting is a technique used to fill in missing or removed parts of an image in a way that is visually coherent with the rest of the image. It's more advanced than simple erasing because the goal of inpainting is to make the edited region look natural, as if the object was never there.</p> <ul> <li>Example: If someone were to erase a stamp (leaving a blank area), inpainting would involve filling that blank area with content (like background texture and color) that matches the surrounding region of the passport, making it appear as if the stamp was never there.</li> </ul> <p> </p> Example: Details have been erased and subsequently filled with background texture and color utilizing AI, making the alterations appear seamless and undetectable.    <p>Each forensic feature is meticulously engineered to provide accurate and insightful analysis, aiding in the identification and understanding of digital manipulations within documents. The combination of these features allows for a comprehensive examination of digital documents to ensure their authenticity and integrity.</p> <p>For further details or inquiries regarding the forensic features, feel free to contact our support team.</p>"},{"location":"fine_tuning/","title":"Model Fine-Tuning","text":""},{"location":"fine_tuning/#fine-tuning-document-tampering-localization-model","title":"Fine-Tuning Document Tampering Localization model:","text":"<p>You can fine-tune the forensics model for the following purposes:</p> <ol> <li> <p>Adding Support for New Card Types: Extend the model's capabilities to handle previously unsupported card types by training it on new datasets specific to those cards.</p> </li> <li> <p>Improving Performance on Existing Card Types: Enhance the accuracy and reliability of tampering detection for currently supported card types by fine-tuning the model with additional high-quality data.</p> </li> </ol>"},{"location":"fine_tuning/#dataset-requirements","title":"Dataset Requirements:","text":"<p>To enhance support for new card types, ensure you collect authentic at least 30 unique, high-quality images of the new card type.</p>"},{"location":"fine_tuning/#tutorials","title":"Tutorials","text":"<p>Watch the video tutorial on fine-tuning the document forensic model:</p> <ul> <li>Google Drive Link </li> <li>GitHub Release Link</li> </ul>"},{"location":"fine_tuning/#steps","title":"Steps:","text":""},{"location":"fine_tuning/#step-1-crop-id-cards","title":"Step 1: Crop ID cards","text":"<p>Crop ID cards to isolate the ID card region from the background, retaining only the ID card region. The current card segmentation model supports PAN cards and Mauritius ID cards. For other card types, you can either search for a pre-trained model online or manually crop the ID card region.</p>"},{"location":"fine_tuning/#command-line-usage","title":"Command Line Usage","text":"<p>This section explains how to use the <code>inference_card_segmentation.py</code> script from the command line.</p> <p>Download the <code>card_segmentation.zip</code> file from the following link: card segmentation model</p> <p>After downloading, extract the contents into the <code>./resources/models</code> folder. Once extracted, the model path will be: <pre><code>./resources/models/card_segmentation/v8.0.1/best.onnx\n</code></pre></p>"},{"location":"fine_tuning/#general-syntax","title":"General Syntax","text":"<pre><code>python inference_card_segmentation.py \\\n   --input_dir \"&lt;path_to_input_directory&gt;\" \\\n   --output_dir \"&lt;path_to_output_directory&gt;\"\n</code></pre> <ul> <li><code>--input_dir</code>: Path to the folder containing input files (e.g., raw ID card images).</li> <li><code>--output_dir</code>: Path to the folder where processed outputs will be saved.</li> </ul>"},{"location":"fine_tuning/#example","title":"Example","text":"<ol> <li>Processing Mauritius ID Cards    <pre><code>python inference_card_segmentation.py \\\n   --input_dir \"datasets/raw/Mauritius-id-nic-v1\" \\\n   --output_dir \"datasets/processed/Mauritius-id-card/card-det-res\"\n</code></pre></li> </ol>"},{"location":"fine_tuning/#step-2-orientation-correction","title":"Step 2: Orientation correction","text":"<p>Correct the orientation of the ID card images, if necessary, to ensure they are upright and properly aligned.</p>"},{"location":"fine_tuning/#step-3-split-the-dataset","title":"Step 3: Split the Dataset","text":"<p>Split the collected images into training and validation sets, using an 80:20 ratio as a default or another ratio of your choice. This means 80% of the images will be used for training, while the remaining 20% will be used for validation. You can create folders named <code>train</code> and <code>val</code> to store the respective data.</p>"},{"location":"fine_tuning/#step-4-generate-tampering-dataset","title":"Step 4: Generate tampering dataset","text":"<p>Generate a tampering dataset by running the data generation script. Ensure to run the script separately for the training and validation sets.</p> <p>The input for the script will be the cropped and split datasets you have prepared earlier. It will produce a tampered image and a corresponding mask for every original image.</p> <p>For each original image, it generates three outputs with specific postfixes added to the filenames:</p> <ol> <li>Original Image: with <code>_orig.jpeg</code> postfix.  </li> <li>Tampered Image: with <code>_tampered.jpeg</code> postfix.  </li> <li>Mask: with <code>_mask.png</code> postfix.</li> </ol> <p>For example, the output files will follow this naming convention:</p> <ul> <li><code>11_orig.jpeg</code>, <code>11_tampered.jpeg</code>, <code>11_mask.png</code></li> <li><code>12_orig.jpeg</code>, <code>12_tampered.jpeg</code>, <code>12_mask.png</code></li> <li><code>13_orig.jpeg</code>, <code>13_tampered.jpeg</code>, <code>13_mask.png</code></li> </ul>"},{"location":"fine_tuning/#command-line-usage_1","title":"Command Line Usage","text":"<p>Use the following syntax and examples to run the script from the command line.</p>"},{"location":"fine_tuning/#general-syntax_1","title":"General Syntax","text":"<pre><code>python data_gen_wrapper.py \n   --source_dir \"&lt;path_to_source_directory&gt;\" \\\n   --out_dir \"&lt;path_to_output_directory&gt;\"\n</code></pre> <ul> <li><code>--source_dir</code>: Path to the source directory containing the cropped and split datasets (e.g., training or validation sets).</li> <li><code>--out_dir</code>: Path to the output directory where the tampered images and masks will be saved.</li> </ul>"},{"location":"fine_tuning/#example-commands","title":"Example Commands","text":"<ol> <li> <p>For generating Training Dataset    <pre><code>python data_gen_wrapper.py \\\n   --source_dir \"./datasets/processed/Mauritius-id-card/split_cropped/train/\" \\\n   --out_dir \"./datasets/data_gen/Mauritius-id-card/train\"\n</code></pre></p> </li> <li> <p>For generating Validation Dataset    <pre><code>python data_gen_wrapper.py \\\n   --source_dir \"./datasets/processed/Mauritius-id-card/split_cropped/val/\" \\\n   --out_dir \"./datasets/forensic_data_gen/Mauritius-id-card/val\"\n</code></pre></p> </li> </ol> <p>By running these commands, tampered images and their corresponding masks will be generated and stored in the specified output directories.</p>"},{"location":"fine_tuning/#step-5-run-model-fine-tuning-script","title":"Step 5: Run Model Fine-Tuning Script","text":"<p>To fine-tune the model, execute the provided <code>main.py</code> script from the command line. This script will fine-tune your model using the specified training and testing datasets.</p>"},{"location":"fine_tuning/#command-line-usage_2","title":"Command Line Usage","text":""},{"location":"fine_tuning/#general-syntax_2","title":"General Syntax","text":"<pre><code>python main.py \\\n    --data_path \"&lt;path_to_training_data&gt;\" \\\n    --test_data_path \"&lt;path_to_testing_data&gt;\" \\\n    --output_dir \"&lt;path_to_model_output&gt;\" \\\n    --weights_path \"&lt;path_to_weights_file&gt;\"\n</code></pre> <ul> <li><code>--data_path</code>: Path to the folder containing the training dataset (e.g., images and masks).</li> <li><code>--test_data_path</code>: Path to the folder containing the testing dataset for evaluation during fine-tuning.</li> <li><code>--output_dir</code>: (Optional) Path to the directory where the fine-tuned model and logs will be saved.</li> <li><code>--weights_path</code>: (Optional) Path to the pre-trained weights file. The default value is <code>./ckpt/v7.0/best_model.pth</code>. You can change this path based on your requirements.</li> </ul>"},{"location":"fine_tuning/#example-command","title":"Example Command","text":"<pre><code>python main.py \\\n    --data_path \"./datasets/data_gen/Mauritius-id-card/train/\" \\\n    --test_data_path \"./datasets/data_gen/Mauritius-id-card/val/\" \\\n    --output_dir \"./output/training/v8.2\" \\\n    --weights_path \"./ckpt/v8.1/best_model.pth\"\n</code></pre>"},{"location":"fine_tuning/#output","title":"Output","text":"<ul> <li><code>Fine-Tuned Model</code>: The updated model file will be saved in the specified <code>--output_dir</code>.</li> <li><code>Fine-Tuning Logs</code>: Logs containing metrics such as loss, accuracy, and validation performance will be saved for further analysis.</li> </ul>"},{"location":"fine_tuning/#notes","title":"Notes","text":"<ol> <li>Ensure that the directories specified in <code>--data_path</code> and <code>--test_data_path</code> contain the correctly prepared datasets.</li> <li>If you have specific pre-trained weights to use, provide the path using <code>--weights_path</code>. Otherwise, the default weights file will be used.</li> <li>The <code>--output_dir</code> should be an empty or new directory to avoid overwriting existing files.</li> </ol>"},{"location":"fine_tuning/#step-6-export-model","title":"Step 6: Export model","text":"<p>Once the fine-tuning process is complete, the trained PyTorch model can be exported to the ONNX (Open Neural Network Exchange) format for compatibility with other frameworks or deployment environments.</p>"},{"location":"fine_tuning/#command-line-usage_3","title":"Command Line Usage","text":""},{"location":"fine_tuning/#general-syntax_3","title":"General Syntax","text":"<pre><code>python export.py --weights_path \"&lt;path_to_trained_model_weights&gt;\" --onnx_output_path \"&lt;path_to_save_onnx_model&gt;\"\n</code></pre> <ul> <li><code>--weights_path</code>: Path to the trained PyTorch model file (e.g., <code>.pth</code> file) that you want to export.</li> <li><code>--onnx_output_path</code>: Path where the exported ONNX model will be saved.</li> </ul>"},{"location":"fine_tuning/#example-command_1","title":"Example Command","text":"<pre><code>python export.py \\\n  --weights_path \"./output/training/v8.2/best_model.pth\" \\\n  --onnx_output_path \"./output/training/v8.2/best_model.onnx\"\n</code></pre>"},{"location":"fine_tuning/#output_1","title":"Output","text":"<ul> <li>ONNX Model File: The exported ONNX model will be saved at the location specified by <code>--onnx_output_path</code>.</li> </ul>"},{"location":"fine_tuning/#notes_1","title":"Notes","text":"<ol> <li>Ensure the <code>--weights_path</code> points to the correct and finalized PyTorch model file from the fine-tuning step.</li> </ol>"},{"location":"input_image_requirements/","title":"Document Forensic - Input Image Requirements","text":"<p>To ensure accurate analysis and reliable results, please follow these image guidelines when using the Document Forensic Project:</p>"},{"location":"input_image_requirements/#1-supported-file-formats-and-extensions","title":"1. Supported File Formats and Extensions","text":"<p>The Document Forensic Project supports the following image file formats and their extensions for analysis:</p> <ul> <li>PNG (<code>.png</code>)  </li> <li>JPG (<code>.jpg</code>)  </li> <li>JPEG (<code>.jpeg</code>)  </li> <li>WEBP (<code>.webp</code>)  </li> <li>TIF (<code>.tif</code>)  </li> <li>TIFF (<code>.tiff</code>)  </li> </ul>"},{"location":"input_image_requirements/#2-resolution-requirements","title":"2. Resolution Requirements:","text":"<ul> <li>Minimum Resolution: 250 x 200 pixels</li> <li>Maximum Resolution:  8200 x 8200 pixels (8K UHD)</li> <li>Recommended Resolution: Full HD (1920 x 1080 pixels)</li> </ul>"},{"location":"input_image_requirements/#3-color-modes","title":"3. Color Modes:","text":"<ul> <li>RGB</li> </ul>"},{"location":"input_image_requirements/#4-aspect-ratio","title":"4. Aspect Ratio:","text":"<ul> <li>Any aspect ratio is acceptable.</li> </ul>"},{"location":"input_image_requirements/#5-maximum-file-size","title":"5. Maximum File Size:","text":"<ul> <li>The maximum file size allowed is <code>10</code> MB.</li> </ul>"},{"location":"input_image_requirements/#6-image-quality","title":"6. Image Quality","text":"<ul> <li>For the most accurate results, use high-quality images.  </li> <li>Avoid submitting images with the following issues, as they may lead to inaccurate analysis:  <ul> <li>Overly bright or dark lighting  </li> <li>Low clarity or resolution  </li> <li>Glare or reflections  </li> <li>Excessive noise or compression artifacts </li> <li>Blurry or out-of-focus images</li> <li>Partially cropped or incomplete documents</li> <li>Skewed or improperly aligned documents</li> <li>Color filtered images</li> <li>Printed and then scanned documents</li> </ul> </li> </ul> <p>For the best results, use high-quality, clear images with a resolution of Full HD (1920 x 1080 pixels). Following these guidelines will enhance the accuracy and reliability of the analysis performed by the Document Forensic Project.</p>"},{"location":"installation/","title":"Installation process of Document Forensics Project","text":"<p>This page will guide you through the initial setup and installation process to get the Document Forensics Project up and running on your machine.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites:","text":"<p>Before proceeding with the installation, ensure you have the following prerequisites met on your machine:</p> <ul> <li> <p>Operating System: </p> <ul> <li>Linux (Ubuntu 18.04 LTS or later is recommended)</li> <li>macOS (10.14 Mojave or later)</li> <li>Windows 10 Pro, Enterprise or Education (64-bit, Build 15063 or later)</li> </ul> </li> <li> <p>Hardware:</p> <ul> <li>CPU: At least 4 cores</li> <li>RAM: 12 GB or more</li> <li>Disk Space: 50 GB or more of free space</li> <li>GPU: A modern nvidia GPU with memory &gt; 12 GB is recommended to provide a significant speedup in predictions.</li> </ul> </li> <li> <p>Software:</p> <ul> <li>Docker: Follow the official Docker installation guide to install Docker on your machine.</li> <li>Docker Compose: Follow the official Docker Compose installation guide to install Docker Compose on your machine.</li> <li>GPU driver: If you are setting up project on nvidia gpu, then driver version &gt;= 525 will be required.</li> <li>Nvidia container toolkit: installation</li> </ul> </li> <li> <p>Others:</p> <ul> <li>Stable internet connection for cloning the repository and downloading dependencies.</li> <li>Access to a terminal or command prompt.</li> </ul> </li> </ul> <p>Ensure that your machine meets the above specifications to ensure smooth operation of the Document Forensics Project.</p>"},{"location":"installation/#installation-guide","title":"Installation Guide:","text":"<p>This guide provides detailed instructions on how to set up the Document Forensics Project on your machine. You can choose to set up the project on a CPU or a GPU, though a GPU setup is recommended due to its advantages in speeding up predictions.</p> <ol> <li>Clone the Repository:    Clone the project repository to your local machine using the following command:</li> </ol> <pre><code>git clone https://github.com/Nivratti/DocForensics_Mauritius_Inference.git\ncd DocForensics_Mauritius_Inference\n</code></pre>"},{"location":"installation/#gpu-setup-recommended","title":"GPU Setup (Recommended):","text":"<p>There are two distinct methods available for setting up the project on a GPU: Docker and Docker Compose. You can opt for any of these methods depending on your preferences or the specifics of your environment setup.</p> <ul> <li>i) Docker Setup:</li> <li>ii) Docker Compose Setup</li> </ul>"},{"location":"installation/#docker-setup","title":"Docker Setup:","text":"<p>This guide provides instructions on setting up the Document Forensics Project on a GPU using Docker.</p> <ol> <li> <p>Build the Docker Image:</p> <p>In the project directory, you'll find a file named <code>Dockerfile</code>. This file contains the instructions to build the Docker image for the GPU setup. Build the Docker image using the following command:</p> <pre><code>docker build -t docforensics_mauritius_inference:latest .\n</code></pre> </li> <li> <p>Run the Docker Container:       Now, run the Docker container from the image you just built. This will start the Document Forensics Project on your machine:</p> <pre><code>docker run --name docforensics_mauritius_inference -p 6003:6003 docforensics_mauritius_inference:latest\n</code></pre> </li> <li> <p>Accessing the Project:       Once the Docker container is up and running, you can access the Document Forensics Project through your web browser using the URL: <code>http://localhost:6003</code></p> </li> <li> <p>Stopping the Project:       To stop the Document Forensics Project and the running container, use the following command:       <pre><code>docker stop docforensics_mauritius_inference\n</code></pre></p> </li> </ol>"},{"location":"installation/#docker-compose-setup","title":"Docker Compose Setup","text":"<p>Docker Compose facilitates the management of multi-container Docker applications, making it a convenient method for setting up the Document Forensics Project on a GPU. Here's how to proceed:</p> <ol> <li> <p>Prepare the Docker Compose File:    Ensure you have a <code>docker-compose.yml</code> file in the project directory. The file should have the content as provided above, tailored to the specifics of the Document Forensics Project and GPU setup.</p> </li> <li> <p>Build the Docker Images:    Navigate to the project directory where the <code>docker-compose.yml</code> file is located, and execute the following command to build the Docker images:    <pre><code>sudo docker-compose build\n</code></pre></p> </li> <li> <p>Start the Docker Containers:    Now, start the Docker containers using the following command:    <pre><code>sudo docker-compose up\n</code></pre></p> </li> <li> <p>Accessing the Project:    Once the Docker containers are up and running, access the Document Forensics Project through your web browser using the URL: <code>http://localhost:6003</code></p> </li> <li> <p>Stopping the Project:    To stop the Document Forensics Project and the running containers, use the following command:    <pre><code>sudo docker-compose down\n</code></pre></p> </li> </ol> <p>These steps outline the procedure for setting up and managing the Document Forensics Project on a GPU using Docker Compose.</p>"},{"location":"installation/#cpu-setup","title":"CPU Setup:","text":"<p>There are two methods available for setting up the project on a CPU: Docker and Docker Compose. You can opt for any of these methods depending on your preferences or the specifics of your environment setup.</p> <ul> <li>i) Docker Setup On CPU:</li> <li>ii) Docker Compose Setup On CPU</li> </ul>"},{"location":"installation/#docker-setup-on-cpu","title":"Docker Setup On CPU:","text":"<p>This guide provides instructions on setting up the Document Forensics Project on a CPU using Docker.</p> <ol> <li> <p>Build the Docker Image:</p> <p>In the project directory, you'll find a file named <code>Dockerfile-cpu</code>. This file contains the instructions to build the Docker image for the CPU setup. Build the Docker image using the following command:</p> <pre><code>sudo docker build -f \"Dockerfile-cpu\" -t docforensics_mauritius_inference_cpu:latest .\n</code></pre> </li> <li> <p>Run the Docker Container:     Now, run the Docker container from the image you just built. This will start the Document Forensics Project on your machine:</p> <pre><code>sudo docker run --name docforensics_mauritius_inference_cpu -p 6003:6003 docforensics_mauritius_inference_cpu:latest\n</code></pre> </li> <li> <p>Accessing the Project:     Once the Docker container is up and running, you can access the Document Forensics Project through your web browser using the URL: <code>http://localhost:6003</code></p> </li> <li> <p>Stopping the Project:     To stop the Document Forensics Project and the running container, use the following command:</p> <pre><code>sudo docker stop docforensics_mauritius_inference_cpu\n</code></pre> </li> </ol>"},{"location":"installation/#docker-compose-setup-on-cpu","title":"Docker Compose Setup On CPU","text":"<p>Docker Compose facilitates the management of multi-container Docker applications, making it a convenient method for setting up the Document Forensics Project on a CPU. Here's how to proceed:</p> <ol> <li> <p>Prepare the Docker Compose File:     Ensure you have a <code>docker-compose-cpu.yml</code> file in the project directory. The file should have the content as provided above, tailored to the specifics of the Document Forensics Project and CPU setup.</p> </li> <li> <p>Build the Docker Images:     Navigate to the project directory where the <code>docker-compose-cpu.yml</code> file is located, and execute the following command to build the Docker images:</p> <pre><code>sudo docker compose -f \"docker-compose-cpu.yml\" build\n</code></pre> </li> <li> <p>Start the Docker Containers:     Now, start the Docker containers using the following command:</p> <pre><code>sudo docker compose -f \"docker-compose-cpu.yml\" up\n</code></pre> </li> <li> <p>Accessing the Project:     Once the Docker containers are up and running, access the Document Forensics Project through your web browser using the URL: <code>http://localhost:6003</code></p> </li> <li> <p>Stopping the Project:     To stop the Document Forensics Project and the running containers, use the following command:</p> <pre><code>sudo docker compose -f \"docker-compose-cpu.yml\" down\n</code></pre> </li> </ol>"},{"location":"installation/#initial-configuration","title":"Initial Configuration:","text":"<p>No initial configuration is required to get started with the Document Forensics Project. Once the installation steps are completed, you can begin using the system to analyze digital documents for forensic purposes.</p> <p>Feel free to explore the project documentation further to understand the various features and functionalities available within the Document Forensics Project.</p>"},{"location":"performance_metrices/","title":"Performance metrices","text":""},{"location":"performance_metrices/#performance-metrics-for-document-forensics","title":"Performance Metrics for Document Forensics","text":"<ol> <li> <p>Accuracy:</p> <ul> <li>Measures the proportion of correctly identified instances among all instances.</li> <li>Formula: ( \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} )</li> </ul> </li> <li> <p>Precision:</p> <ul> <li>Evaluates the proportion of true positive predictions among all positive predictions.</li> <li>Formula: ( \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}} )</li> </ul> </li> <li> <p>Recall (Sensitivity):</p> <ul> <li>Assesses the proportion of true positive predictions among all actual positives.</li> <li>Formula: ( \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}} )</li> </ul> </li> <li> <p>F1-Score:</p> <ul> <li>Provides a balance between precision and recall.</li> <li>Formula: ( \\text{F1-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} )</li> </ul> </li> <li> <p>False Positive Rate (FPR):</p> <ul> <li>Measures the proportion of false positives among all actual negatives.</li> <li>Formula: ( \\text{FPR} = \\frac{\\text{False Positives}}{\\text{False Positives} + \\text{True Negatives}} )</li> </ul> </li> <li> <p>False Discovery Rate (FDR):</p> <ul> <li>Evaluates the proportion of false positive predictions among all positive predictions.</li> <li>Formula: ( \\text{FDR} = \\frac{\\text{False Positives}}{\\text{True Positives} + \\text{False Positives}} )</li> </ul> </li> <li> <p>Matthews Correlation Coefficient (MCC):</p> <ul> <li>Provides an overall measure of quality for binary classifications.</li> <li>Formula: ( \\text{MCC} = \\frac{(\\text{TP} \\cdot \\text{TN}) - (\\text{FP} \\cdot \\text{FN})}{\\sqrt{((\\text{TP} + \\text{FP})(\\text{TP} + \\text{FN})(\\text{TN} + \\text{FP})(\\text{TN} + \\text{FN}))}} )</li> </ul> </li> <li> <p>Area Under the Receiver Operating Characteristic Curve (AUC-ROC):</p> <ul> <li>Evaluates the performance of a binary classification model.</li> <li>The closer the AUC-ROC value is to 1, the better the model is at distinguishing between the positive and negative classes.</li> </ul> </li> <li> <p>Confusion Matrix:</p> <ul> <li>A table that is used to evaluate the performance of a classification model.</li> <li>Shows true positive, true negative, false positive, and false negative values which can be used to calculate other performance metrics.</li> </ul> </li> <li> <p>Mean Squared Error (MSE):</p> <ul> <li>Measures the average squared differences between the estimated values and the actual values.</li> <li>Formula: ( \\text{MSE} = \\frac{1}{n} \\sum (y_{\\text{true}} - y_{\\text{pred}})^2 )</li> </ul> </li> </ol>"},{"location":"release_notes/","title":"Release Notes","text":"<p>This page provides an overview of the changes made in each version of the Document Forensics project, including new features, improvements, and bug fixes.</p>"},{"location":"release_notes/#version-history","title":"Version History","text":""},{"location":"release_notes/#v120-2023-10-06","title":"[v1.2.0] - 2023-10-06","text":""},{"location":"release_notes/#new-features","title":"New Features:","text":"<ul> <li>Introduced API endpoints for forensic analysis.</li> <li>Added Gradio UI for easier interaction.</li> <li>Enhanced inpainting detection algorithms.</li> </ul>"},{"location":"release_notes/#improvements","title":"Improvements:","text":"<ul> <li>Improved performance and speed of the splicing detection.</li> <li>Optimized the project setup using Docker and Docker Compose.</li> </ul>"},{"location":"release_notes/#bug-fixes","title":"Bug Fixes:","text":"<ul> <li>Fixed an issue where the erasing detection was not accurate in certain cases.</li> <li>Addressed a bug that caused incorrect labeling in the Copy-Move detection.</li> </ul>"},{"location":"release_notes/#v110-2023-08-15","title":"[v1.1.0] - 2023-08-15","text":""},{"location":"release_notes/#new-features_1","title":"New Features:","text":"<ul> <li>Added support for more image formats including 'webp'.</li> <li>Introduced a new setup script for easier deployment.</li> </ul>"},{"location":"release_notes/#improvements_1","title":"Improvements:","text":"<ul> <li>Enhanced the accuracy of the Copy-Move detection.</li> <li>Improved the documentation for better understanding of the setup process.</li> </ul>"},{"location":"release_notes/#bug-fixes_1","title":"Bug Fixes:","text":"<ul> <li>Fixed a bug that caused the project to fail when processing large images.</li> <li>Addressed an issue where the Docker container would not start due to missing dependencies.</li> </ul>"},{"location":"release_notes/#v100-2023-02-17","title":"[v1.0.0] - 2023-02-17","text":""},{"location":"release_notes/#new-features_2","title":"New Features:","text":"<ul> <li>Initial release with basic forensic features including Copy-Move, Splicing, Erasing, Masking, and Inpainting detection.</li> <li>Provided Docker and Docker Compose setup for easy deployment.</li> </ul>"},{"location":"understanding_document_forensics_workflow/","title":"Understanding Document Forensics Workflow","text":""},{"location":"understanding_document_forensics_workflow/#how-the-solution-classifies-an-image","title":"How the Solution Classifies an Image:","text":"<p>The solution employs a trained model to identify the forgery regions within an image. These regions are illustrated in a mask, with white representing forged pixels and black denoting untouched pixels. The extent of white pixels serves as a basis to categorize an image as original, suspicious, or tampered.</p>"},{"location":"understanding_document_forensics_workflow/#steps","title":"Steps:","text":"<ol> <li> <p>Image Preparation:</p> <ul> <li>The image is loaded, resized to a resolution of 512x512 pixels, and all pixel values are normalized to a range of 0 to 1.</li> </ul> </li> <li> <p>Model Prediction:</p> <ul> <li>The trained deep learning model scrutinizes the image and generates a prediction mask in grayscale. This mask accentuates areas where tampering or forgery is presumed to have taken place.</li> </ul> </li> <li> <p>A \"mask\" in this context is an image of identical dimensions to the input, but instead of displaying the actual content, it unveils the regions (if any) identified as tampered.</p> </li> <li> <p>Mask Processing:</p> <ul> <li>To render decisions based on the mask, certain thresholds are applied to transition the grayscale mask to a monochromatic format (black and white). Pixel values below 128 are converted to 0 (indicating black), and pixel values above 128 are converted to 255 (indicating white).</li> </ul> </li> <li> <p>Forgery Extent Calculation:</p> <ul> <li>The proportion of the mask that is white (symbolic of forgery) is computed.</li> </ul> </li> <li> <p>Image Classification:</p> <ul> <li>Original: Classified if nearly no white pixels are present.</li> <li>Suspicious: The image might encompass some regions of forgery, but it's not definitive. If the white pixel percentage ranges between 0 to 0.57%, it will be labeled as suspicious.</li> <li>Tampered: Classified if the percentage of white pixels surpasses the upper threshold, i.e., if white pixels percentage is more than 0.57%.</li> </ul> </li> </ol> <p>Note: The thresholds for \"Suspicious\" and \"Tampered\" classifications can be adjusted as needed. These thresholds have been established based on the model's performance on Mauritius ID and Pancard images.</p>"},{"location":"understanding_document_forensics_workflow/#example-images-and-predicted-forgery-mask","title":"Example images and predicted forgery mask:","text":"# Image Predicted Mask Label 1 Original Image 2 Original Image 3 Tampered Image (Masking - text redaction) 4 Tampered Image (Copy-move) 5 Tampered Image (Splicing - face region changed)"},{"location":"user_guide/","title":"User Guide","text":"<p>This user guide provides detailed instructions on how to use the Document Forensic Project solution to analyze digital documents for potential tampering. The solution offers two interfaces for user access: Gradio UI and API interface.</p> <ul> <li>User Guide</li> <li>Access via Gradio UI</li> <li>Access via API Interface</li> </ul>"},{"location":"user_guide/#access-via-gradio-ui","title":"Access via Gradio UI","text":"<p>The Gradio UI provides a user-friendly interface to upload and analyze digital documents.</p> <p></p> <ol> <li> <p>Uploading Documents:</p> <ul> <li>Navigate to the Gradio UI.</li> <li>You can either drag and drop the document file onto the designated area or click to browse and select the file from your device.</li> </ul> </li> <li> <p>Submitting Documents for Analysis:</p> <ul> <li>Once the document file is selected, click on the <code>Submit</code> button to initiate the analysis.</li> </ul> </li> <li> <p>Viewing Analysis Results:</p> <ul> <li>Prediction Result: Label indicating the document's status as either <code>Original</code>, <code>Suspicious</code>, or <code>Tampered</code>.</li> <li>Stats: A key-value pair format displaying statistics of the predicted mask.</li> <li>Result Mask: A black and white monochrome mask generated from the grayscale predicted mask. White regions highlight the tampered or altered areas, while black regions signify untouched or genuine areas.</li> <li>Result Color Mask: An RGB color mask providing a colored representation of the analyzed areas.</li> <li>Document Integrity Verification Result: Key-value pairs indicating detected objects, along with their confidence levels.</li> </ul> </li> </ol>"},{"location":"user_guide/#access-via-api-interface","title":"Access via API Interface","text":"<p>The API interface provides a programmatic way to submit documents for analysis and retrieve the results.</p> <ol> <li> <p>API Endpoint:</p> <ul> <li>The API endpoint for document submission is <code>http://localhost:6003/api/predict</code>.</li> </ul> </li> <li> <p>Submitting Documents for Analysis:</p> <ul> <li>Make a POST request to the API endpoint with the document file attached in the request body.</li> </ul> </li> <li> <p>Viewing Analysis Results:</p> <ul> <li>The analysis results will be returned in the response body, including Prediction Result, Stats, Result Mask, Result Color Mask, and Document Integrity Verification Result, similar to the Gradio UI.</li> </ul> </li> </ol> <p>For more information on the API parameters and response format, refer to the API documentation.</p>"}]}